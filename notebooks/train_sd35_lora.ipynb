{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a57637c",
   "metadata": {},
   "source": [
    "# Train a LoRA for Stable Diffusion 3.5 (SD3.5)\n",
    "\n",
    "Goal: fine-tune SD 3.5 to your dataset using a **LoRA** (Diffusers + Accelerate).\n",
    "\n",
    "This notebook is designed to be robust across Diffusers versions by auto-discovering a suitable SD3 LoRA training script under `diffusers/examples/`.\n",
    "\n",
    "You’ll do:\n",
    "- check/install dependencies\n",
    "- point to your dataset (`images/` + `metadata.jsonl`)\n",
    "- run training with `accelerate`\n",
    "- load SD 3.5 and apply the trained LoRA for inference\n",
    "\n",
    "Tip: training requires a GPU for practical runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a41c2a",
   "metadata": {},
   "source": [
    "## 1) Environment & dependency setup\n",
    "\n",
    "If you are training SD 3.5, you generally need a relatively recent `torch` + `diffusers` version.\n",
    "\n",
    "If you don't want to disturb this repo’s environment, create a separate env and run this notebook there.\n",
    "\n",
    "This notebook will:\n",
    "- print versions\n",
    "- optionally upgrade/install packages in the current kernel (you control this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197b9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Nov  6 2025, 13:44:16) [GCC 13.3.0]\n",
      "Platform: Linux-6.14.0-37-generic-x86_64-with-glibc2.39\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def run(cmd: list[str]) -> None:\n",
    "    print(' '.join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498b6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.9.1+cu128\n",
      "diffusers==0.37.0.dev0\n",
      "transformers==4.57.3\n",
      "accelerate==1.12.0\n",
      "peft==0.11.1\n",
      "safetensors==0.7.0\n",
      "datasets: NOT INSTALLED (No module named 'datasets')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.9.1+cu128\n",
      "diffusers==0.37.0.dev0\n",
      "transformers==4.57.3\n",
      "accelerate==1.12.0\n",
      "peft==0.11.1\n",
      "safetensors==0.7.0\n",
      "datasets: NOT INSTALLED (No module named 'datasets')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print key package versions (if installed)\n",
    "import os\n",
    "\n",
    "# Avoid diffusers failing import due to strict PEFT version gating in some source checkouts.\n",
    "os.environ.setdefault('_CHECK_PEFT', '0')\n",
    "\n",
    "def try_import(name: str):\n",
    "    try:\n",
    "        m = __import__(name)\n",
    "        v = getattr(m, '__version__', 'unknown')\n",
    "        print(f'{name}=={v}')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'{name}: NOT INSTALLED ({e})')\n",
    "        return False\n",
    "\n",
    "try_import('torch')\n",
    "try_import('diffusers')\n",
    "try_import('transformers')\n",
    "try_import('accelerate')\n",
    "try_import('peft')\n",
    "try_import('safetensors')\n",
    "try_import('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11816b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: install/upgrade training packages in this kernel.\n",
    "# If you already have SD 3.5 running in your other notebook, you may skip this.\n",
    "#\n",
    "# Recommended (GPU): torch + torchvision should match your CUDA setup.\n",
    "# For example, for CUDA 12.1 you might use the PyTorch CUDA wheels index URL.\n",
    "#\n",
    "# WARNING: This may conflict with the repo's pinned requirements.\n",
    "INSTALL_IN_KERNEL = False  # set True if you want this notebook to pip-install deps\n",
    "\n",
    "if INSTALL_IN_KERNEL:\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--upgrade',\n",
    "         'diffusers', 'transformers', 'accelerate', 'peft', 'safetensors', 'datasets', 'torchvision'])\n",
    "    # bitsandbytes is optional (useful for low-VRAM / 8-bit optimizers on some setups)\n",
    "    # run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'bitsandbytes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf4286",
   "metadata": {},
   "source": [
    "## 2) Hugging Face access (token)\n",
    "\n",
    "If SD 3.5 requires gated access for your account, you need to:\n",
    "1) accept the model terms on Hugging Face\n",
    "2) provide a token to this environment\n",
    "\n",
    "This notebook will use `HF_TOKEN` (or `HUGGINGFACE_HUB_TOKEN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5fdb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No HF token found in env. You can set it like:\n",
      "  export HF_TOKEN=hf_...\n",
      "Then restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = (\n",
    "    os.environ.get('HF_TOKEN')\n",
    "    or os.environ.get('HUGGINGFACE_HUB_TOKEN')\n",
    "    or os.environ.get('HUGGINGFACE_TOKEN')\n",
    "    or os.environ.get('HF_API_TOKEN')\n",
    ")\n",
    "\n",
    "if HF_TOKEN is None:\n",
    "    print('No HF token found in env. You can set it like:')\n",
    "    print('  export HF_TOKEN=hf_...')\n",
    "    print('Then restart the kernel.')\n",
    "else:\n",
    "    print('HF token found in environment (hidden).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1b67c",
   "metadata": {},
   "source": [
    "## 3) Dataset format (images + captions / DreamBooth LoRA)\n",
    "\n",
    "This notebook supports two common training styles:\n",
    "\n",
    "1) **Captioned LoRA** (image+text pairs): you provide `images/` + `metadata.jsonl` with one caption per image.\n",
    "2) **DreamBooth LoRA** (subject-driven): you provide **only `images/`**, and a single `--instance_prompt` is used for all images.\n",
    "\n",
    "### Captioned LoRA (image + caption)\n",
    "Recommended simple structure (local folder):\n",
    "```\n",
    "data/my_lora_dataset/\n",
    "  images/\n",
    "    0001.png\n",
    "    0002.png\n",
    "    ...\n",
    "  metadata.jsonl\n",
    "```\n",
    "Where `metadata.jsonl` contains one JSON object per line, e.g.:\n",
    "```json\n",
    "{\"file_name\": \"0001.png\", \"text\": \"a photo of a <my_subject> in a forest\"}\n",
    "{\"file_name\": \"0002.png\", \"text\": \"a <my_subject> on a table, studio lighting\"}\n",
    "```\n",
    "Using a unique token like `<my_subject>` helps steer the LoRA concept.\n",
    "\n",
    "### DreamBooth LoRA (instance images + instance prompt)\n",
    "For DreamBooth LoRA, captions in `metadata.jsonl` are **not used**. The script trains from `--instance_data_dir` (your `images/` folder) and a single `--instance_prompt` like:\n",
    "- `a photo of sks dog`\n",
    "- `a photo of <my_subject>`\n",
    "\n",
    "If you need per-image captions, use the captioned LoRA path instead of DreamBooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73563278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/data/shepplogan\n",
      "Images dir exists: True\n",
      "Metadata exists: True\n",
      "metadata.jsonl lines: 100\n",
      "sample 0 {'file_name': '0001.png', 'text': 'A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, significantly enlarged ventricles, normal white matter, minimal lesions'}\n",
      "sample 1 {'file_name': '0002.png', 'text': 'A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, enlarged ventricles, normal white matter, visible lesions'}\n",
      "sample 2 {'file_name': '0003.png', 'text': 'A synthetic CT-style Shepp-Logan brain phantom showing large brain, enlarged ventricles, expanded white matter, prominent lesions'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATASET_ROOT = Path('../data/shepplogan')  # change this\n",
    "IMAGES_DIR = DATASET_ROOT / 'images'\n",
    "METADATA_JSONL = DATASET_ROOT / 'metadata.jsonl'\n",
    "\n",
    "print('Dataset root:', DATASET_ROOT.resolve())\n",
    "print('Images dir exists:', IMAGES_DIR.exists())\n",
    "print('Metadata exists:', METADATA_JSONL.exists())\n",
    "\n",
    "if METADATA_JSONL.exists():\n",
    "    # quick validation / preview\n",
    "    lines = METADATA_JSONL.read_text(encoding='utf-8').splitlines()\n",
    "    print('metadata.jsonl lines:', len(lines))\n",
    "    for i, line in enumerate(lines[:3]):\n",
    "        obj = json.loads(line)\n",
    "        print('sample', i, obj)\n",
    "        fp = IMAGES_DIR / obj['file_name']\n",
    "        if not fp.exists():\n",
    "            print('  WARNING: missing image:', fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c4179",
   "metadata": {},
   "source": [
    "## 5) Configure training parameters\n",
    "\n",
    "Fill in the following values for your setup.\n",
    "- `MODEL_ID`: SD 3.5 checkpoint on Hugging Face\n",
    "- `OUTPUT_DIR`: where LoRA weights will be saved\n",
    "- `RESOLUTION`: choose based on GPU VRAM (e.g., 512 is a common starting point)\n",
    "- `MAX_TRAIN_STEPS`: increase for more learning; start small to validate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96e270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ID: stabilityai/stable-diffusion-3.5-medium\n",
      "OUTPUT_DIR: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/notebooks/outputs/sd35_lora_my_dataset\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = 'stabilityai/stable-diffusion-3.5-medium'\n",
    "OUTPUT_DIR = Path('outputs/sd35_lora_my_dataset')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Core training knobs\n",
    "RESOLUTION = 128\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_TRAIN_STEPS = 10\n",
    "\n",
    "# LoRA rank (higher = more capacity, more VRAM/size)\n",
    "LORA_RANK = 8\n",
    "\n",
    "# Mixed precision can help speed + VRAM on modern GPUs. Use 'bf16' if supported, else 'fp16'.\n",
    "MIXED_PRECISION = 'bf16'\n",
    "\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0790c",
   "metadata": {},
   "source": [
    "## 6) Create a non-interactive Accelerate config\n",
    "\n",
    "`accelerate config` is usually interactive. In notebooks, we generate a simple config file automatically.\n",
    "You can adjust it if you need multi-GPU or specific distributed settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18437c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote accelerate config: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/notebooks/outputs/accelerate_config.yaml\n",
      "compute_environment: LOCAL_MACHINE\n",
      "distributed_type: 'NO'\n",
      "mixed_precision: bf16\n",
      "num_machines: 1\n",
      "num_processes: 1\n",
      "machine_rank: 0\n",
      "gpu_ids: all\n",
      "use_cpu: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "ACCELERATE_CONFIG = Path('outputs/accelerate_config.yaml')\n",
    "ACCELERATE_CONFIG.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Minimal single-process config (good default for 1 GPU).\n",
    "cfg = {\n",
    "    'compute_environment': 'LOCAL_MACHINE',\n",
    "    'distributed_type': 'NO',\n",
    "    'mixed_precision': MIXED_PRECISION,\n",
    "    'num_machines': 1,\n",
    "    'num_processes': 1,\n",
    "    'machine_rank': 0,\n",
    "    'gpu_ids': 'all',\n",
    "    'use_cpu': False,\n",
    "}\n",
    "\n",
    "ACCELERATE_CONFIG.write_text(yaml.safe_dump(cfg, sort_keys=False), encoding='utf-8')\n",
    "print('Wrote accelerate config:', ACCELERATE_CONFIG.resolve())\n",
    "print(ACCELERATE_CONFIG.read_text(encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645b50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/notebooks\n",
      "Repo root: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite\n",
      "DreamBooth LoRA TRAIN_SCRIPT: /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/train_dreambooth_lora_sd3.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / 'pyproject.toml').exists() or (p / '.git').exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "def find_train_dreambooth_lora_sd3_script() -> str | None:\n",
    "    repo_root = find_repo_root()\n",
    "\n",
    "    # Prefer the repo-local training script (no downloads).\n",
    "    candidates = [\n",
    "        repo_root / 'train_dreambooth_lora_sd3.py',\n",
    "        # Also accept a local diffusers checkout if present\n",
    "        repo_root / 'diffusers/examples/dreambooth/train_dreambooth_lora_sd3.py',\n",
    "        repo_root.parent / 'diffusers/examples/dreambooth/train_dreambooth_lora_sd3.py',\n",
    "        repo_root.parent.parent / 'diffusers/examples/dreambooth/train_dreambooth_lora_sd3.py',\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return str(c)\n",
    "\n",
    "    # Last resort: search from repo root (skip common heavy dirs).\n",
    "    skip_parts = {'.git', '__pycache__', '.venv', 'venv', 'site-packages'}\n",
    "    matches: list[Path] = []\n",
    "    for p in repo_root.rglob('train_dreambooth_lora_sd3.py'):\n",
    "        if any(part in skip_parts for part in p.parts):\n",
    "            continue\n",
    "        matches.append(p)\n",
    "\n",
    "    matches.sort(key=lambda p: (p.name != 'train_dreambooth_lora_sd3.py', len(p.parts)))\n",
    "    if matches:\n",
    "        return str(matches[0])\n",
    "\n",
    "    return None\n",
    "\n",
    "print('CWD:', Path.cwd())\n",
    "print('Repo root:', find_repo_root())\n",
    "TRAIN_SCRIPT = find_train_dreambooth_lora_sd3_script()\n",
    "print('DreamBooth LoRA TRAIN_SCRIPT:', TRAIN_SCRIPT)\n",
    "if TRAIN_SCRIPT is None:\n",
    "    raise RuntimeError(\n",
    "        'Could not find train_dreambooth_lora_sd3.py in this repo. '\n",
    "        'Expected it at repo root as ./train_dreambooth_lora_sd3.py.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbd640",
   "metadata": {},
   "source": [
    "## 7) Launch LoRA training (SD3/SD3.5)\n",
    "\n",
    "This section uses Diffusers’ SD3 DreamBooth LoRA example script `train_dreambooth_lora_sd3.py`.\n",
    "\n",
    "Two modes are supported:\n",
    "\n",
    "- **Captioned (per-image prompts)**: if `metadata.jsonl` exists, we try to train using those captions by passing `--dataset_name` + `--caption_column` to the script.\n",
    "- **DreamBooth (single prompt)**: if no captions are found, we train from `--instance_data_dir` and a single `--instance_prompt`.\n",
    "\n",
    "Note: the script still requires `--instance_prompt` even in captioned mode; it’s used as a fallback if a caption is missing/empty.\n",
    "\n",
    "If training fails with an “unrecognized arguments” error, scroll up to the error and remove/rename the flagged args (the training script usually has `--help`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f717f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using captions from metadata.jsonl: True\n",
      "INSTANCE_PROMPT (fallback): A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, significantly enlarged ventricles, normal white matter, minimal lesions\n",
      "VALIDATION_PROMPT: A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, significantly enlarged ventricles, normal white matter, minimal lesions\n",
      "Training command (copy/paste):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shlex\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if TRAIN_SCRIPT is None:\n",
    "    raise RuntimeError(\n",
    "        'TRAIN_SCRIPT is None. Re-run the DreamBooth script discovery cell in step 7.'\n",
    "    )\n",
    "\n",
    "# If you have per-image prompts in metadata.jsonl, we will prefer captioned training.\n",
    "USE_CAPTIONS = METADATA_JSONL.exists()\n",
    "\n",
    "# The DreamBooth SD3 script expects captions to come from a Hugging Face Datasets dataset column.\n",
    "# For the common imagefolder-style metadata.jsonl used in this repo, the caption key is typically 'text'.\n",
    "CAPTION_COLUMN = 'text'\n",
    "\n",
    "first_caption = None\n",
    "if USE_CAPTIONS:\n",
    "    try:\n",
    "        first_line = METADATA_JSONL.read_text(encoding='utf-8').splitlines()[0]\n",
    "        first_obj = json.loads(first_line)\n",
    "        # Try a few common keys\n",
    "        first_caption = (\n",
    "            first_obj.get('text')\n",
    "            or first_obj.get('caption')\n",
    "            or first_obj.get('prompt')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Warning: could not read first caption from metadata.jsonl:', e)\n",
    "        first_caption = None\n",
    "\n",
    "# IMPORTANT: the script requires --instance_prompt even in captioned mode (fallback when a caption is missing).\n",
    "# If we have captions, default to the first caption so you don't have to repeat yourself.\n",
    "INSTANCE_PROMPT = (first_caption or 'a photo of <my_subject>')  # change if needed\n",
    "\n",
    "# Validation prompt is only used to periodically *sample* images during training.\n",
    "# In captioned mode we default it to the first caption; you can override to anything you want.\n",
    "VALIDATION_PROMPT = (first_caption or INSTANCE_PROMPT)  # optional but recommended\n",
    "\n",
    "# Choose how to point the script to your data:\n",
    "# - captioned mode: use --dataset_name so the script can read captions via `datasets`\n",
    "# - dreambooth mode: use --instance_data_dir with a single prompt\n",
    "INSTANCE_DATA_DIR = str(IMAGES_DIR)\n",
    "DATASET_NAME = str(DATASET_ROOT)\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, '-m', 'accelerate.commands.launch',\n",
    "    '--config_file', str(ACCELERATE_CONFIG),\n",
    "    TRAIN_SCRIPT,\n",
    "    '--pretrained_model_name_or_path', MODEL_ID,\n",
    "    '--output_dir', str(OUTPUT_DIR),\n",
    "    '--mixed_precision', MIXED_PRECISION,\n",
    "    '--resolution', str(RESOLUTION),\n",
    "    '--train_batch_size', str(TRAIN_BATCH_SIZE),\n",
    "    '--gradient_accumulation_steps', str(GRADIENT_ACCUMULATION_STEPS),\n",
    "    '--learning_rate', str(LEARNING_RATE),\n",
    "    '--lr_scheduler', 'constant',\n",
    "    '--lr_warmup_steps', '0',\n",
    "    '--max_train_steps', str(MAX_TRAIN_STEPS),\n",
    "    '--rank', str(LORA_RANK),\n",
    "    '--validation_prompt', VALIDATION_PROMPT,\n",
    "    '--validation_epochs', '25',\n",
    "    '--seed', '42',\n",
    "    '--report_to', 'tensorboard',\n",
    "    '--logging_dir', str(OUTPUT_DIR / 'logs'),\n",
    "    '--gradient_checkpointing',\n",
    " ]\n",
    "\n",
    "if USE_CAPTIONS:\n",
    "    # Uses `datasets.load_dataset(DATASET_NAME, ...)` under the hood.\n",
    "    # This requires `datasets` installed and your DATASET_ROOT to be in a format datasets can load.\n",
    "    cmd += [\n",
    "        '--dataset_name', DATASET_NAME,\n",
    "        '--caption_column', CAPTION_COLUMN,\n",
    "    ]\n",
    "else:\n",
    "    cmd += [\n",
    "        '--instance_data_dir', INSTANCE_DATA_DIR,\n",
    "    ]\n",
    "\n",
    "# Always required by the script (fallback in captioned mode).\n",
    "cmd += [\n",
    "    '--instance_prompt', INSTANCE_PROMPT,\n",
    " ]\n",
    "\n",
    "# Keep token in env as well (some scripts infer it from env).\n",
    "env = os.environ.copy()\n",
    "if HF_TOKEN is not None:\n",
    "    env['HF_TOKEN'] = HF_TOKEN\n",
    "    env['HUGGINGFACE_HUB_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Avoid diffusers failing import due to strict PEFT version gating in some source checkouts.\n",
    "env['_CHECK_PEFT'] = '0'\n",
    "\n",
    "print('Using captions from metadata.jsonl:', USE_CAPTIONS)\n",
    "print('INSTANCE_PROMPT (fallback):', INSTANCE_PROMPT)\n",
    "print('VALIDATION_PROMPT:', VALIDATION_PROMPT)\n",
    "print('Training command (copy/paste):')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c97cfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/bin/python -m accelerate.commands.launch --config_file outputs/accelerate_config.yaml /home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/train_dreambooth_lora_sd3.py --pretrained_model_name_or_path stabilityai/stable-diffusion-3.5-medium --output_dir outputs/sd35_lora_my_dataset --mixed_precision bf16 --resolution 128 --train_batch_size 1 --gradient_accumulation_steps 4 --learning_rate 0.0001 --lr_scheduler constant --lr_warmup_steps 0 --max_train_steps 10 --rank 8 --validation_prompt 'A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, significantly enlarged ventricles, normal white matter, minimal lesions' --validation_epochs 25 --seed 42 --report_to tensorboard --logging_dir outputs/sd35_lora_my_dataset/logs --gradient_checkpointing --dataset_name ../data/shepplogan --caption_column text --instance_prompt 'A synthetic CT-style Shepp-Logan brain phantom showing normal-sized brain, significantly enlarged ventricles, normal white matter, minimal lesions'\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(shlex.quote(x) for x in cmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c01bc4",
   "metadata": {},
   "source": [
    "## 8) Find the saved LoRA weights\n",
    "\n",
    "Diffusers commonly writes LoRA weights as a `.safetensors` file under `OUTPUT_DIR`.\n",
    "We’ll search for likely LoRA weight files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b890db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found weight candidates: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found weight candidates: 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno No LoRA weight files found under OUTPUT_DIR. Did training run and write weights? OUTPUT_DIR=/home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/notebooks/outputs/sd35_lora_my_dataset] : 'Expected patterns: **/*lora*.safetensors, **/*lora*.bin, **/*lora*.pt, **/*pytorch_lora_weights*.safetensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m, p)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m weight_candidates:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mNo LoRA weight files found under OUTPUT_DIR. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mDid training run and write weights? \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mOUTPUT_DIR=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR.resolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     18\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mExpected patterns: **/*lora*.safetensors, **/*lora*.bin, **/*lora*.pt, **/*pytorch_lora_weights*.safetensors\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     21\u001b[39m LORA_WEIGHTS = weight_candidates[\u001b[32m0\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAuto-selected LORA_WEIGHTS:\u001b[39m\u001b[33m'\u001b[39m, LORA_WEIGHTS)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno No LoRA weight files found under OUTPUT_DIR. Did training run and write weights? OUTPUT_DIR=/home/simone/Dropbox/eScience_projects/xAI/Attend-and-Excite/notebooks/outputs/sd35_lora_my_dataset] : 'Expected patterns: **/*lora*.safetensors, **/*lora*.bin, **/*lora*.pt, **/*pytorch_lora_weights*.safetensors'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "weight_candidates = []\n",
    "for pattern in ['**/*lora*.safetensors', '**/*lora*.bin', '**/*lora*.pt', '**/*pytorch_lora_weights*.safetensors']:\n",
    "    weight_candidates.extend(glob.glob(str(OUTPUT_DIR / pattern), recursive=True))\n",
    "\n",
    "weight_candidates = sorted(set(weight_candidates))\n",
    "print('Found weight candidates:', len(weight_candidates))\n",
    "for p in weight_candidates:\n",
    "    print('-', p)\n",
    "\n",
    "if not weight_candidates:\n",
    "    raise FileNotFoundError(\n",
    "        'No LoRA weight files found under OUTPUT_DIR. '\n",
    "        'Did training run and write weights? '\n",
    "        f'OUTPUT_DIR={OUTPUT_DIR.resolve()}','',\n",
    "        'Expected patterns: **/*lora*.safetensors, **/*lora*.bin, **/*lora*.pt, **/*pytorch_lora_weights*.safetensors',\n",
    "    )\n",
    "\n",
    "LORA_WEIGHTS = weight_candidates[0]\n",
    "print('Auto-selected LORA_WEIGHTS:', LORA_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d58606",
   "metadata": {},
   "source": [
    "## 9) Inference: load SD 3.5 + apply LoRA\n",
    "\n",
    "This uses Diffusers `AutoPipelineForText2Image`.\n",
    "Depending on your Diffusers version, LoRA APIs can be `load_lora_weights()` / `set_adapters()` / `fuse_lora()`.\n",
    "This notebook uses the most common `load_lora_weights()` flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5278a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36acbffb1dc04c5d98122ac95d519b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10898542ff144c98867cd6a17f19ad83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00001-of-00002.safe(…):   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafb8665bd2485f97586adc8fdf7763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder_3/model-00002-of-00002.safe(…):   0%|          | 0.00/4.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fed3790ad94d67a0eb2d93869e3328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformer/diffusion_pytorch_model.safe(…):   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m dtype = (\n\u001b[32m     12\u001b[39m     torch.bfloat16\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (torch.cuda.is_available() \u001b[38;5;129;01mand\u001b[39;00m MIXED_PRECISION == \u001b[33m'\u001b[39m\u001b[33mbf16\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (torch.float16 \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m torch.float32)\n\u001b[32m     15\u001b[39m  )\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mDevice:\u001b[39m\u001b[33m'\u001b[39m, device, \u001b[33m'\u001b[39m\u001b[33mdtype:\u001b[39m\u001b[33m'\u001b[39m, dtype)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m pipe = \u001b[43mAutoPipelineForText2Image\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m.to(device)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LORA_WEIGHTS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNo LoRA weights selected yet. Set LORA_WEIGHTS to a .safetensors path from step 8.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/diffusers/src/diffusers/pipelines/auto_pipeline.py:485\u001b[39m, in \u001b[36mAutoPipelineForText2Image.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_or_path, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m text_2_image_cls = _get_task_class(AUTO_TEXT2IMAGE_PIPELINES_MAPPING, orig_class_name)\n\u001b[32m    484\u001b[39m kwargs = {**load_config_kwargs, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_2_image_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/diffusers/src/diffusers/pipelines/pipeline_utils.py:829\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    825\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    826\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe provided pretrained_model_name_or_path \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    828\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     cached_folder = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    848\u001b[39m     cached_folder = pretrained_model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/diffusers/src/diffusers/pipelines/pipeline_utils.py:1747\u001b[39m, in \u001b[36mDiffusionPipeline.download\u001b[39m\u001b[34m(cls, pretrained_model_name, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# download all allow_patterns - ignore_patterns\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     cached_folder = \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1757\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1759\u001b[39m     cls_name = \u001b[38;5;28mcls\u001b[39m.load_config(os.path.join(cached_folder, \u001b[33m\"\u001b[39m\u001b[33mmodel_index.json\u001b[39m\u001b[33m\"\u001b[39m)).get(\u001b[33m\"\u001b[39m\u001b[33m_class_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1760\u001b[39m     cls_name = cls_name[\u001b[32m4\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cls_name, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cls_name.startswith(\u001b[33m\"\u001b[39m\u001b[33mFlax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m cls_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    330\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/eScience_projects/xAI/Attend-and-Excite/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Avoid diffusers failing import due to strict PEFT version gating in some source checkouts.\n",
    "os.environ.setdefault('_CHECK_PEFT', '0')\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = (\n",
    "    torch.bfloat16\n",
    "    if (torch.cuda.is_available() and MIXED_PRECISION == 'bf16')\n",
    "    else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    " )\n",
    "print('Device:', device, 'dtype:', dtype)\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    token=HF_TOKEN,\n",
    ").to(device)\n",
    "\n",
    "if LORA_WEIGHTS is None:\n",
    "    print('No LoRA weights selected yet. Set LORA_WEIGHTS to a .safetensors path from step 8.')\n",
    "else:\n",
    "    lora_path = Path(LORA_WEIGHTS)\n",
    "    print('Loading LoRA weights:', lora_path)\n",
    "    pipe.load_lora_weights(str(lora_path.parent), weight_name=lora_path.name)\n",
    "    # Optional: fuse LoRA for faster inference (not supported on all versions/pipelines)\n",
    "    try:\n",
    "        pipe.fuse_lora()\n",
    "        print('Fused LoRA into the pipeline.')\n",
    "    except Exception as e:\n",
    "        print('fuse_lora() not available (ok):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa516d2e",
   "metadata": {},
   "source": [
    "## 10) Generate an image (SD 3.5 + LoRA)\n",
    "\n",
    "Edit the prompt/token to match how you captioned your dataset (e.g., `<my_subject>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45283120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROMPT = 'a photo of <my_subject>, cinematic lighting'\n",
    "NEGATIVE_PROMPT = None\n",
    "SEED = 1234\n",
    "STEPS = 30\n",
    "GUIDANCE = 7.0\n",
    "\n",
    "gen = (\n",
    "    torch.Generator(device=device).manual_seed(SEED)\n",
    "    if device == 'cuda'\n",
    "    else torch.Generator().manual_seed(SEED)\n",
    " )\n",
    "\n",
    "out = pipe(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    num_inference_steps=STEPS,\n",
    "    guidance_scale=GUIDANCE,\n",
    "    generator=gen,\n",
    " )\n",
    "\n",
    "img = out.images[0]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('SD 3.5 + LoRA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4732f",
   "metadata": {},
   "source": [
    "## Practical tips / common failure modes\n",
    "\n",
    "- VRAM OOM: lower `RESOLUTION`, keep `TRAIN_BATCH_SIZE=1`, increase `GRADIENT_ACCUMULATION_STEPS`, keep `--gradient_checkpointing`.\n",
    "- If `TRAIN_SCRIPT` args don’t match: run `python TRAIN_SCRIPT --help` and adjust the `cmd` list.\n",
    "- If the model is gated: confirm you accepted SD 3.5 terms on Hugging Face and that `HF_TOKEN` is set (restart kernel after exporting).\n",
    "- If outputs ignore your concept: improve captions (consistent token like `<my_subject>`), increase steps a bit, or raise `LORA_RANK` slightly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
